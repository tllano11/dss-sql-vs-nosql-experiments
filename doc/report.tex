\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{filecontents}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{geometry}
\geometry{
  a4paper,
  total={170mm,257mm},
  left=20mm,
  top=20mm,
}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  breaklines=true,
  showstringspaces=false  
}

\begin{filecontents}{bibliography.bib}
  @misc{rhelVM,
    author = {{RedHat}},
    title = {{Product Documentation for Red Hat Enterprise Linux 6.0}},
    chapter = {{Tuning Virtual Memory}},
    howpublished = {\url{https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-tunables}}
  }
  @misc{tariqOOM,
    title = {{OOM} relation to vm.swappiness=0 in new kernel},
    howpublished = {\url{https://www.percona.com/blog/2014/04/28/oom-relation-vm-swappiness0-new-kernel/}},
    year = 2014,
    month = Apr,
    day = 28,
    note = {Accessed: 2019-02-23}
  }
  @misc{mongoTHP,
    author = {{mongoDB}},
    title = {{MongoDB Performance}},
    chapter = {{Disable Transparent Huge Pages (THP)}},
    howpublished = {\url{https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/}}
  }
  @misc{psql041519,
    author = {{PostgreSQL}},
    title = {{Managing Kernel Resources}},
    chapter = {{Linux Memory Overcommit}},
    howpublished = {\url{https://www.postgresql.org/docs/current/kernel-resources.html#LINUX-MEMORY-OVERCOMMIT/}}
  }
\end{filecontents}

\title{Independent Study - Report}
\author{Tomas F. Llano Rios}
\begin{document}
\maketitle
\section{Summary}

The goal of the indepent study is to compare the efficiency of a relational system (PostgreSQL) and a 
NoSQL system (mongodb) when working with semi-structured data. To this end, queries 1, 3, 4, 12, 13 and
22 of the TPC-H benchmark were run in both database engines: The measurements obtained from PostgreSQL
represent a baseline, while those obtained from mongodb help to stablish a performance comparison.\\
The activities proposed to perform the investigation, as well as their status at the time this report
is being written, are summarized in Table~\ref{tab:activities}. To monitor the progress and completion of
each activity, weekly meetings with professors Antonio Badia and Mohammed Khalifa were setup each
Tuesday at 12:30m.\\
The report is structured as follows: Section~\ref{sec:dataset_generation} describes how the TPC-H
data is produced, Section~\ref{sec:optimizations} lists the system and engines' options tuned to
maximize performance, Section~\ref{sec:evaluation} des\-cribes how the queries were run, recorded and
showed, and Section~\ref{sec:troubleshooting} explains in detail the major problems that
arose during the investigation. 

\begin{table}[t]
\center
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Activity}}                                                                                                                                                          & \multicolumn{1}{c|}{\textbf{Status}} \\ \hline
Install mongodb                                                                                                                                                                                  & Completed                            \\ \hline
Install PostgreSQL                                                                                                                                                                               & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Rewrite queries in mongodb's query language\\ for data aggregated in a single json collection:\\ customer $\bowtie$ orders $\bowtie$ lineitem\end{tabular}                      & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Rewrite queries in mongodb's query language\\ for data aggregated in two json collections:\\ one for customer and other for \\ orders $\bowtie$ lineitem\end{tabular}      & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Rewrite queries in mongodb's query language\\ for data aggregated in three json collections:\\ one for customer, one for orders, and \\ one for lineitem\end{tabular} & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Rewrite queries in PostgreSQL JSON query \\ language\end{tabular}                                                                                                     & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Generate TPCH data for PostgreSQL and \\ PostgreSQL JSON\end{tabular}                                                                                                 & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Generate TPCH data for mongodb using\\ a single collection: \\ customer $\bowtie$ orders $\bowtie$ lineitem\end{tabular}                                                        & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Generate TPCH data for mongodb using\\ two collections: \\ customer, and orders $\bowtie$ lineitem\end{tabular}                                                            & Completed                            \\ \hline
\begin{tabular}[c]{@{}l@{}}Generate TPCH data for mongodb using\\ a three collections: \\ customer, orders, and lineitem\end{tabular}                                                            & completed                            \\ \hline
Time queries and record results                                                                                                                                                                  & Completed                            \\ \hline
\end{tabular}
\caption{List of activities proposed as part of the independent study}
\label{tab:activities}
\end{table}

\section{Dataset generation}\label{sec:dataset_generation}

\subsection{PostgreSQL}\label{ssec:gen_tpch_psql}

\begin{enumerate}
\item Execute \texttt{./dbgen -s <scale factor>} inside \texttt{/path/to/TPC\_H/dbgen/}.
\item Remove the extra `\texttt{|}' at the end of each line from each .tbl file
  produced in the previous step.
  \begin{lstlisting}
    for i in `ls *.tbl`; do sed -i 's/|$//' $i; done
  \end{lstlisting}
\item Import the data into PostgreSQL (see Annex~\ref{ap:import_tpch_psql}).
\end{enumerate}

\subsection{mongodb}
Generating the TPCH benchmark dataset and convert it to mongodb's extended json format is relatively easy 
for ``small'' scales, less than 25 in this case: 
\begin{enumerate}

\item Follow steps from Section~\ref{ssec:gen_tpch_psql}.
\item Run a SQL query on PostgreSQL to transform the relational data into
json (see Annex~\ref{ap:gen_tpch_mongodb}).
\end{enumerate}

Once data is large enough, creating the json by using PostgreSQL becomes difficult because of the
memory rapidly running out. This is mainly due to the amount of data kept in RAM when the
database engine is aggregating information into json. As such, a C++ program
was created to read records from the tables directly from disk, aggregate them,
write each json to disk and remove it from memory afterwards (see Section~\ref{ssec:oomerrs} for more details). 
The program uses a slightly modified  version of the sort-merge join 
algorithm (see Annex~\ref{ap:sort_merge_join}), thus it assumes the input to be sorted in a certain
way:

\begin{enumerate}
\item Sort order by custkey
  \begin{lstlisting}
    sort -kn 2 -t '|' orders.tbl > orders_sorted.csv
  \end{lstlisting}

\item Sort lineitems by orderkey taken into consideration the new order produced from the
  previous step. Because orderkey can be in an arbitrary order after \texttt{orders.csv} 
  is sorted by custkey, the unique values of orderkey are first extracted and assigned a number based on the order 
  they appear (what awk is used for); next, the numbers are added to lineitems and sorted; 
  finally, the extra column is removed (what cut is for).
  \begin{lstlisting}
    awk -F'|' 'NR==FNR{o[$1]=FNR; next} {print o[$1] ``|'' $0}' \
    <(awk -F'|' '{print $1}' orders_sorted.csv | uniq) \
    lineitem.tbl | sort -t '|' -nk1 | cut -d'|' -f2- > \
    lineitem_sorted.csv
  \end{lstlisting}
\end{enumerate}

\section{Optimizations}\label{sec:optimizations}

\subsection{PostgreSQL optimizations}
The following values were set in
\texttt{/var/lib/pgsql/10/data/postgresql.conf}:

\begin{lstlisting}
shared_buffers = 11776MB
maintenance_work_mem = 2GB
effective_io_concurrency = 2
max_worker_processes = 16
max_parallel_workers_per_gather = 8
max_parallel_workers = 16
wal_buffers = 16MB
max_wal_size = 8GB
min_wal_size = 4GB
checkpoint_completion_target = 0.9
effective_cache_size = 35GB # 3/4 of total ram
default_statistics_target = 5000
\end{lstlisting}

\subsection{System optimizations}

\subsubsection{Swappiness}

Avoid swapping to disk by decreasing swapiness to 10. Note it could be set to 0, but until
confirming such value will not cause the OOM Killer to kill PostgreSQL or MongoDB processes
(see \cite{rhelVM} and \cite{tariqOOM})
it is safer to have some disk swapping.
\begin{lstlisting}[language=bash]
sysctl -w vm.swappiness=10
\end{lstlisting}

\subsubsection{dirty pages}

Reduce percentage of system memory used to hold dirty pages. As recommended by RedHat\cite{rhelVM}
\texttt{dirty\_ratio} should be lowered to 15 (or less depending on the system's capacity) for
database workloads and \texttt{dirty\_background\_ratio} to 3. The commands below ensure such
values to be set temporarily (until there is a reboot).
\begin{lstlisting}[language=bash]
sysctl -w vm.dirty_ratio=15
sysctl -w vm.dirty_background_ratio=3
\end{lstlisting}

\subsubsection{Transparent HugePages}

Due to how MongoDB uses memory, this feature (introduced in RHEL6) should be disbled:
``(..) database workloads often perform poorly with THP, because they tend to have sparse rather
than contiguous memory access patterns.''\cite{mongoTHP}.
\begin{verbatim}
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/defrag
\end{verbatim}

\subsubsection{Overcommit memory}
Applications may acquire too much memory and not release it, which invokes the OOM killer.
Avoiding overcommiting beyond the overcommit ratio lowers, althogh does not ensure, the
chances of the OOM killer being invoked. See \cite{psql041519}.

\begin{verbatim}
sysctl -w vm.overcommit_memory=2
\end{verbatim}

\subsubsection{NUMA}

MongoDB is not NUMA aware, therefore it's recommended to start \texttt{mongod} using interleaved-mode.

\begin{verbatim}
sed -i 's,^ExecStart=.*,ExecStart=/usr/bin/numactl \
--interleave=all /usr/bin/mongod $OPTIONS,' \
/usr/lib/systemd/system/mongod.service
systemctl daemon-reload
systemctl restart mongod
\end{verbatim}

\section{Evaluation}\label{sec:evaluation}

Queries 1,3,4,12,13 and 22 of the TPC-H benchmark were used to compare the efficiency of PSQL and mongodb,
as these queries only involve the usage of tables customer, orders and/or lineitem. Because there is a
one to many relationship between customer and orders, and orders and lineitem, the claim of mongodb
being more efficient due to not requiring joins can effectively be tested.\\

For the test to be reliable, two nested structures were created inside mongodb:

\begin{enumerate}
  \item Representing customer $\bowtie$ orders $\bowtie$ lineitem
    \begin{lstlisting}
      { <cust1 attributes>, orders: [ {<order attributes>, lineitems:[{<lineitem attributes>}, ...]}, ...] 
    \end{lstlisting}
  \item Representing orders $\bowtie$ lineitem
    \begin{lstlisting}
      {<order attributes>, lineitems:[{<lineitem attributes>}, ...]}
    \end{lstlisting}
\end{enumerate}

and all tables were also imported individually in order to test mongodb's join capabilities. As such, there
are five different categories for each query (that is, there are 5 different versions of a query for a total
of $6\times 5 = 30$ queries):

\begin{itemize}
\item \textbf{psql}: Queries under this category are used as a baseline and are given by the TPC-H
  benchmark.
\item \textbf{psql json}: Queries under this category run on a pure nested json structure
  like the one used for the ``1 collection'' category.
\item \textbf{1 collection}: Queries under this category run over the json representation of
  customer $\bowtie$ orders $\bowtie$ lineitem.
\item \textbf{2 collections}: Queries under this category run over the json representation of
  orders $\bowtie$ lineitem and perform a join with customer.
\item \textbf{3 collections}: Queries under this category perform a join of the three independent 
  collections: customer, orders and lineitem.
\end{itemize}

Queries were run using a bash script (see Annex~\ref{ap:runexps}). The program executes each query five (5)
times and stores the output in a text file, after which it calculates the average running time and
saves the results in a space-delimited csv file using the format:

\begin{lstlisting}
<query 1> <average running time>
<query 2> <average running time>
<query 3> <average running time>
...
<query n> <average running time>
\end{lstlisting}

Once the script finishes, a chart for different categories of the same query is manually created 
(with, for instance, libreoffice). This procedure was repeated for three scales: 1G, 10G and 25G.\\

Finally, indexes were created in both mongodb and PostgreSQL to evaluate how running time would
vary in both cases. Naturally, this required the original variables of the queries to be changed
in order to:
\begin{enumerate}
\item Guarantee better selectivity and force the database engines to use the indexes
\item Ensure consistency when running the tests without indexes
\end{enumerate}

\section{Troubleshooting}\label{sec:troubleshooting}

\subsection{Out Of Memory (OOM) errors at json creation}\label{ssec:oomerrs}

After surpassing certain scale (10G in this case) the process running the SQL query exporting data in JSON format
is killed because of an Out Of Memory (OOM) error (this can be confirmed by executing (as the root user): \texttt{dmesg --level=err}
and obtaining a message such as: \texttt{[8165398.216541] Out of memory: Kill process 26462 (psql) score 732 or sacrifice child}).
This is to be expected because the json representation of the data is much larger (e.g. a scale factor of 25G
produces a json of approximately 67G). As such, the solution considered as the most adecuate was to write a 
program able to aggregate the data, write every document once it is created and free the resources needed to hold
such document in memory. An example of this can be seen on Annex~\ref{ap:sort_merge_join}; every time a customer is written
to disk, the structure being kept in memory to hold their information is deleted.

\subsection{Comparison operators on nested arrays}

The mongodb operator \texttt{\$lt} allows to compare fields from a json object against
constant values or other fields. However, when used with nested
documents the operator only uses the first document in the list without considering
the others. An example is shown below:

\begin{enumerate}

\item Consider the data below:

\begin{verbatim}
db.tests.insertMany([
    {
        _id: 1,
        data: [
            {a: ISODate("1995-08-06T00:00:00Z"), 
             b: ISODate("2000-08-06T00:00:00Z")},
            {a: ISODate("2018-08-06T00:00:00Z"), 
             b: ISODate("2015-08-06T00:00:00Z")}
        ]
    },
    {
        _id: 2,
        data: [
            {a: ISODate("2018-08-06T00:00:00Z"), 
             b: ISODate("2015-08-06T00:00:00Z")},
            {a: ISODate("1995-08-06T00:00:00Z"), 
             b: ISODate("2000-08-06T00:00:00Z")}
        ]
    },
    {
        _id: 3,
        data: [
            {a: ISODate("1995-08-06T00:00:00Z"), 
             b: ISODate("2000-08-06T00:00:00Z")},
            {a: ISODate("2005-08-06T00:00:00Z"), 
             b: ISODate("2015-08-06T00:00:00Z")}
        ]

    },
    {
        _id: 4,
        data: [
            {a: ISODate("2015-08-06T00:00:00Z"), 
             b: ISODate("2000-08-06T00:00:00Z")},
            {a: ISODate("2016-08-06T00:00:00Z"), 
             b: ISODate("2015-08-06T00:00:00Z")}
        ]

    }

])
\end{verbatim}

\item Consider the query below:

\begin{verbatim}
db.tests.aggregate(
    {$project: {
        _id: 0,
        a_lt_b:{$lt:["$data.a", "$data.b"]}
    }}                                                   
)
\end{verbatim}

\item Executing it results in:

\begin{verbatim}
{ "a_lt_b" : true }
{ "a_lt_b" : false }
{ "a_lt_b" : true }
{ "a_lt_b" : false }
\end{verbatim}

Which means \texttt{\$lt} does not evaluate to all elements inside the array. If it did, 
results should be based on whether \texttt{a < b} for all elements or \texttt{a < b} for at least 
one element. If we consider the prior, document one is a contradiction as it should have 
evaluated to false; if we consider the latter, document 1 and 2 should have both evaluated to true. 
In other words, \texttt{\$lt} is only considering the first element of the array.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bibliography.bib}

\appendix
\section{Importing data into postgresql} \label{ap:import_tpch_psql}
\subsection{Import data}
\begin{lstlisting}
DROP DATABASE IF EXISTS tpch;

CREATE DATABASE tpch;
\c tpch

CREATE SCHEMA <schema>;

/*
Create tables
  -  Adapted from TPCH's dss.ddl file v2.1.8.1 
*/
CREATE TABLE <schema>.NATION  ( N_NATIONKEY  INTEGER NOT NULL,
                            N_NAME       CHAR(25) NOT NULL,
                            N_REGIONKEY  INTEGER NOT NULL,
                            N_COMMENT    VARCHAR(152));

CREATE TABLE <schema>.REGION  ( R_REGIONKEY  INTEGER NOT NULL,
                            R_NAME       CHAR(25) NOT NULL,
                            R_COMMENT    VARCHAR(152));

CREATE TABLE <schema>.PART  ( P_PARTKEY     INTEGER NOT NULL,
                          P_NAME        VARCHAR(55) NOT NULL,
                          P_MFGR        CHAR(25) NOT NULL,
                          P_BRAND       CHAR(10) NOT NULL,
                          P_TYPE        VARCHAR(25) NOT NULL,
                          P_SIZE        INTEGER NOT NULL,
                          P_CONTAINER   CHAR(10) NOT NULL,
                          P_RETAILPRICE DECIMAL(15,2) NOT NULL,
                          P_COMMENT     VARCHAR(23) NOT NULL );

CREATE TABLE <schema>.SUPPLIER ( S_SUPPKEY     INTEGER NOT NULL,
                             S_NAME        CHAR(25) NOT NULL,
                             S_ADDRESS     VARCHAR(40) NOT NULL,
                             S_NATIONKEY   INTEGER NOT NULL,
                             S_PHONE       CHAR(15) NOT NULL,
                             S_ACCTBAL     DECIMAL(15,2) NOT NULL,
                             S_COMMENT     VARCHAR(101) NOT NULL);

CREATE TABLE <schema>.PARTSUPP ( PS_PARTKEY     INTEGER NOT NULL,
                             PS_SUPPKEY     INTEGER NOT NULL,
                             PS_AVAILQTY    INTEGER NOT NULL,
                             PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL,
                             PS_COMMENT     VARCHAR(199) NOT NULL );

CREATE TABLE <schema>.CUSTOMER ( C_CUSTKEY     INTEGER NOT NULL,
                             C_NAME        VARCHAR(25) NOT NULL,
                             C_ADDRESS     VARCHAR(40) NOT NULL,
                             C_NATIONKEY   INTEGER NOT NULL,
                             C_PHONE       CHAR(15) NOT NULL,
                             C_ACCTBAL     DECIMAL(15,2)   NOT NULL,
                             C_MKTSEGMENT  CHAR(10) NOT NULL,
                             C_COMMENT     VARCHAR(117) NOT NULL);

CREATE TABLE <schema>.ORDERS  ( O_ORDERKEY       INTEGER NOT NULL,
                           O_CUSTKEY        INTEGER NOT NULL,
                           O_ORDERSTATUS    CHAR(1) NOT NULL,
                           O_TOTALPRICE     DECIMAL(15,2) NOT NULL,
                           O_ORDERDATE      DATE NOT NULL,
                           O_ORDERPRIORITY  CHAR(15) NOT NULL,  
                           O_CLERK          CHAR(15) NOT NULL, 
                           O_SHIPPRIORITY   INTEGER NOT NULL,
                           O_COMMENT        VARCHAR(79) NOT NULL);

CREATE TABLE <schema>.LINEITEM ( L_ORDERKEY    INTEGER NOT NULL,
                             L_PARTKEY     INTEGER NOT NULL,
                             L_SUPPKEY     INTEGER NOT NULL,
                             L_LINENUMBER  INTEGER NOT NULL,
                             L_QUANTITY    DECIMAL(15,2) NOT NULL,
                             L_EXTENDEDPRICE  DECIMAL(15,2) NOT NULL,
                             L_DISCOUNT    DECIMAL(15,2) NOT NULL,
                             L_TAX         DECIMAL(15,2) NOT NULL,
                             L_RETURNFLAG  CHAR(1) NOT NULL,
                             L_LINESTATUS  CHAR(1) NOT NULL,
                             L_SHIPDATE    DATE NOT NULL,
                             L_COMMITDATE  DATE NOT NULL,
                             L_RECEIPTDATE DATE NOT NULL,
                             L_SHIPINSTRUCT CHAR(25) NOT NULL,
                             L_SHIPMODE     CHAR(10) NOT NULL,
                             L_COMMENT      VARCHAR(44) NOT NULL);

/*
Copy data from CSV files to tables.
  - Note integrity constraints (primary key, foreign key and index) were not specified.
    This is because they might slow down the insertion process
*/
BEGIN;
COPY <schema>.NATION FROM '/path/to/nation.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
BEGIN;
COPY <schema>.REGION FROM '/path/to/region.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
sBEGIN;
COPY <schema>.PART FROM '/path/to/part.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
COPY <schema>.SUPPLIER FROM '/path/to/supplier.csv' WITH (FORMAT csv, DELIMITER '|');
BEGIN;
COPY <schema>.PARTSUPP FROM '/path/to/partsupp.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
BEGIN;
COPY <schema>.CUSTOMER FROM '/path/to/customer.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
BEGIN;
COPY <schema>.ORDERS FROM '/path/to/orders.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
BEGIN;
COPY <schema>.LINEITEM FROM '/path/to/lineitem.csv' WITH (FORMAT csv, DELIMITER '|');
COMMIT;
\end{lstlisting}

\subsection{Add integrity contraints}
\begin{lstlisting}
\c tpch
/*
Add Integrity constraints
  - Adapted from TPCH's dss.ri file v2.1.8.1 
*/
--For table REGION
ALTER TABLE SCALE1.REGION
ADD PRIMARY KEY (R_REGIONKEY);

-- For table NATION
ALTER TABLE SCALE1.NATION
ADD PRIMARY KEY (N_NATIONKEY);

ALTER TABLE SCALE1.NATION
ADD FOREIGN KEY (N_REGIONKEY) references SCALE1.REGION;

COMMIT WORK;

-- For table PART
ALTER TABLE SCALE1.PART
ADD PRIMARY KEY (P_PARTKEY);

COMMIT WORK;

-- For table SUPPLIER
ALTER TABLE SCALE1.SUPPLIER
ADD PRIMARY KEY (S_SUPPKEY);

ALTER TABLE SCALE1.SUPPLIER
ADD FOREIGN KEY (S_NATIONKEY) references SCALE1.NATION;

COMMIT WORK;

-- For table PARTSUPP
ALTER TABLE SCALE1.PARTSUPP
ADD PRIMARY KEY (PS_PARTKEY,PS_SUPPKEY);

COMMIT WORK;

-- For table CUSTOMER
ALTER TABLE SCALE1.CUSTOMER
ADD PRIMARY KEY (C_CUSTKEY);

ALTER TABLE SCALE1.CUSTOMER
ADD FOREIGN KEY (C_NATIONKEY) references SCALE1.NATION;

COMMIT WORK;

-- For table LINEITEM
ALTER TABLE SCALE1.LINEITEM
ADD PRIMARY KEY (L_ORDERKEY,L_LINENUMBER);

COMMIT WORK;

-- For table ORDERS
ALTER TABLE SCALE1.ORDERS
ADD PRIMARY KEY (O_ORDERKEY);

COMMIT WORK;

-- For table PARTSUPP
ALTER TABLE SCALE1.PARTSUPP
ADD FOREIGN KEY (PS_SUPPKEY) references SCALE1.SUPPLIER;

COMMIT WORK;

ALTER TABLE SCALE1.PARTSUPP
ADD FOREIGN KEY (PS_PARTKEY) references SCALE1.PART;

COMMIT WORK;

-- For table ORDERS
ALTER TABLE SCALE1.ORDERS
ADD FOREIGN KEY (O_CUSTKEY) references SCALE1.CUSTOMER;

COMMIT WORK;

-- For table LINEITEM
ALTER TABLE SCALE1.LINEITEM
ADD FOREIGN KEY (L_ORDERKEY)  references SCALE1.ORDERS;


COMMIT WORK;

ALTER TABLE SCALE1.LINEITEM
ADD FOREIGN KEY  (L_PARTKEY,L_SUPPKEY) references 
        SCALE1.PARTSUPP;

COMMIT WORK;
\end{lstlisting}

\section{Generating TPCH dataset for mongodb} \label{ap:gen_tpch_mongodb}

\subsection{For 1 collection}
\begin{lstlisting}
\c tpch
-- Remove headers
\t on
-- Remove spacing between the results
\pset format unaligned
-- Convert relational data to JSON
select row_to_json(r) from
       (select 
       	       c_custkey,
	       c_name,
	       c_address,
	       c_nationkey,
	       c_phone::text,
	       c_acctbal,
	       c_mktsegment::text,
	       c_comment,
               coalesce(json_agg(json_build_object(
                        'o_orderkey', o.o_orderkey,
                        'o_orderstatus', o.o_orderstatus,
                        'o_totalprice', o.o_totalprice,
                        'o_orderdate', json_build_object('$date', to_char(o.o_orderdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                        'o_orderpriority', o.o_orderpriority::text,
                        'o_clerk', o.o_clerk::text,
                        'o_shippriority', o.o_shippriority,
                        'o_comment', o.o_comment,
                        'o_lineitems', o.o_lineitems
               )) filter (where o.o_orderkey is not null), '[]') as c_orders
        from 
             <schema>.customer c left outer join
             (select o.*,
                     coalesce(json_agg(json_build_object(
                               'l_partkey', l.l_partkey,
                               'l_suppkey', l.l_suppkey,
                               'l_linenumber', l.l_linenumber,
                               'l_quantity', l.l_quantity,
                               'l_extendedprice', l.l_extendedprice,
                               'l_discount', l.l_discount,
                               'l_tax', l.l_tax,
                               'l_returnflag', l.l_returnflag,
                               'l_linestatus', l.l_linestatus,
                               'l_shipdate', json_build_object('$date', to_char(l.l_shipdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                               'l_commitdate', json_build_object('$date', to_char(l.l_commitdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                               'l_receiptdate', json_build_object('$date', to_char(l.l_receiptdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                               'l_shipinstruct', l.l_shipinstruct::text,
                               'l_shipmode', l.l_shipmode::text,
                               'l_comment', l.l_comment    
                     )) filter (where l.l_linenumber is not null), '[]') as o_lineitems
              from 
                   <schema>.orders o left outer join <schema>.lineitem l
                   on o.o_orderkey = l.l_orderkey 
              group by 
                    o.o_orderkey, 
                    o_orderstatus, 
                    o_totalprice, 
                    o_orderdate, 
                    o_orderpriority, 
                    o_clerk, 
                    o_shippriority, 
                    o_comment, 
                    o.o_custkey) o
             on c.c_custkey = o.o_custkey 
        group by 
              c.c_custkey, 
              c_name, 
              c_address, 
              c_nationkey, 
              c_phone, 
              c_acctbal, 
              c_mktsegment, 
              c_comment) r \g json_data.json
\end{lstlisting}

\subsection{For 2 collections}

\begin{lstlisting}
\c tpch
-- Remove headers
\t on
-- Remove spacing between the results
\pset format unaligned
-- Convert relational data to JSON
select row_to_json(o) from
     (select 
             o_orderkey as _id,
             o_custkey,
             o_orderstatus,
             o_totalprice,
             json_build_object('$date', to_char(o.o_orderdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')) as o_orderdate,
             o_orderpriority::text,
             o_clerk::text,
             o_shippriority,
             o_comment,
             coalesce(json_agg(json_build_object(
                       'l_partkey', l.l_partkey,
                       'l_suppkey', l.l_suppkey,
                       'l_linenumber', l.l_linenumber,
                       'l_quantity', l.l_quantity,
                       'l_extendedprice', l.l_extendedprice,
                       'l_discount', l.l_discount,
                       'l_tax', l.l_tax,
                       'l_returnflag', l.l_returnflag,
                       'l_linestatus', l.l_linestatus,
                       'l_shipdate', json_build_object('$date', to_char(l.l_shipdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                       'l_commitdate', json_build_object('$date', to_char(l.l_commitdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                       'l_receiptdate', json_build_object('$date', to_char(l.l_receiptdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')),
                       'l_shipinstruct', l.l_shipinstruct::text,
                       'l_shipmode', l.l_shipmode::text,
                       'l_comment', l.l_comment    
             )) filter (where l.l_linenumber is not null), '[]') as o_lineitems
      from 
           <schema>.orders o left outer join <schema>.lineitem l
           on o.o_orderkey = l.l_orderkey 
      group by 
            _id, 
	    o_custkey,
            o_orderstatus, 
            o_totalprice, 
            o_orderdate, 
            o_orderpriority, 
            o_clerk, 
            o_shippriority, 
            o_comment, 
            o_custkey) o \g json_data_order-litems.json

select row_to_json(c) from
     (select 
      	       c_custkey as _id,
	       c_name,
	       c_address,
	       c_nationkey,
	       c_phone::text,
	       c_acctbal,
	       c_mktsegment::text,
	       c_comment
      from
               <schema>.customer) c \g json_data_customer.json

\end{lstlisting}

\subsection{For 3 collections}

\begin{lstlisting}
\c tpch
-- Remove headers
\t on
-- Remove spacing between the results
\pset format unaligned
-- Convert relational data to JSON
select row_to_json(l) from
    (select
             json_build_object(
			'l_orderkey', l_orderkey,
			'l_linenumber', l_linenumber
             ) as _id,
             l_partkey,
             l_suppkey,
             l_quantity,
             l_extendedprice,
             l_discount,
             l_tax,
             l_returnflag,
             l_linestatus,
             json_build_object('$date', to_char(l_shipdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')) as l_shipdate,
             json_build_object('$date', to_char(l_commitdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')) as l_commitdate,
             json_build_object('$date', to_char(l_receiptdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')) as l_receiptdate,
             l_shipinstruct::text,
             l_shipmode::text,
             l_comment    

     from
             <schema>.lineitem) l \g json_data_lineitem.json

select row_to_json(o) from
    (select
             o_orderkey as _id,
             o_custkey,
             o_orderstatus,
             o_totalprice,
             json_build_object('$date', to_char(o_orderdate,'YYYY-MM-DD"T"HH24:MI:SS"Z"')) as o_orderdate,
             o_orderpriority::text,
             o_clerk::text,
             o_shippriority,
             o_comment
     from
             <schema>.orders) o \g json_data_orders.json

 select row_to_json(c) from
      (select 
       	       c_custkey as _id,
 	       c_name,
 	       c_address,
 	       c_nationkey,
 	       c_phone::text,
 	       c_acctbal,
 	       c_mktsegment::text,
 	       c_comment
       from
                <schema>.customer) c \g json_data_customer.json
\end{lstlisting}
\newpage
\section{Sort-Merge Join}\label{ap:sort_merge_join}
\begin{figure}[htpb]
\center
\includegraphics[scale=0.55]{sort-merge_join.png}
\end{figure}

\section{Script used to run queries}\label{ap:runexps}
\begin{lstlisting}
#!/bin/bash
WDIR="$PWD"
ODIR="$PWD/$$"
QUERIES_DIR="../../queries/tpch"
RUNS=5

PGDIR='/usr/pgsql-10/bin'
PGDATA='/var/lib/pgsql/10/data'
PGUSER='postgres'
PGSERVICE='postgresql-10'

function abort {
    local lineno=$1
    local msg=$2
    echo >&2 '
***************
*** ABORTED ***
***************
'
    echo "An error occurred at line $lineno:" >&2
    echo "$msg" >&2
    exit 1
}

function usage {
    echo "usage: $0 -q queries [-s sourcedir] [-o outdir] [-r runs]"
    echo "       $0 -h"
    echo
    echo "  -q queries     queries to run list of comma separated numbers. e.g. 3,4,12,13,22"
    echo "  -s sourcedir   path to directory from where to get queries"
    echo "                 filenames must match *qQUERYNUMBER(v[0-9]+)?_*"
    echo "                   e.g. File containing Q1:"
    echo "                        q1_something.sql"
    echo "                   e.g. File containing version 2 of Q1:"
    echo "                        q1v2_something.js"
    echo "                 (Default: $QUERIES_DIR)"
    echo "  -o outdir      path to directory to where results will be written"
    echo "                 (Default: $ODIR)"
    echo "  -r runs        Runs per query. Used to compute average runtime"
    echo "                 (Default: $RUNS)"
    echo "  -r runs        Runs per query. Used to compute average runtime"
    echo "  -h             Print this mesage"
}

function drop_psql_cache {
    password=$1
    cd /tmp
    # Stop PostgreSQL server
    echo $password | sudo -S  -u $PGUSER $PGDIR/pg_ctl stop -D $PGDATA
    # Drop system caches
    echo $password | sudo -S  bash -c 'sync && echo 3 > /proc/sys/vm/drop_caches'
    # Restart server
    echo $password | sudo -S  systemctl start $PGSERVICE
    # Wait for server to restart
    while ! echo $password | sudo -S  -u $PGUSER $PGDIR/pg_ctl status -D $PGDATA | grep -q 'server is running'; do
	sleep 3
    done
    cd $WDIR
}

function drop_mongo_cache {
    password=$1
    cd /tmp
    echo $password | sudo -S  systemctl stop mongod
    # Drop system caches
    echo $password | sudo -S  bash -c 'sync && echo 3 > /proc/sys/vm/drop_caches'
    echo $password | sudo -S  systemctl start mongod
    cd $WDIR
}

q_selected=false
while getopts ":o:s:q:r:h" opt; do
    case $opt in
	o)
	    ODIR="$OPTARG"
	    ;;
	s)
	    QUERIES_DIR="$OPTARG"
	    ;;
	q)
	    q_selected=true
	    IFS=, read -a QUERIES_TO_RUN <<< "$OPTARG"
	    ;;
	r)
	    RUNS=$OPTARG
	    ;;
	h)
	    usage
	    ;;
	\?)
	    echo "Unknown option: -$OPTARG" >&2; exit 1;;
	:)
	    echo "Missing argument for option: -$OPTARG" >&2; exit 1;;
    esac
done

if ! $q_selected; then
    echo "Please specify the queries to be run (-q option)" >&2
    echo "Use option -h to print usage" >&2    
    exit 1
fi

mkdir -p $ODIR
if [ $? -ne 0 ]; then
    echo "Could not create output dir: $ODIR" >&2
    exit 1
fi

echo "Enter sudo password:"
read -s password
echo "Enter PGPASSWORD"
read -s pgpassword

trap 'abort ${LINENO} "$BASH_COMMAND"' ERR
set -e

for q in ${QUERIES_TO_RUN[@]}; do
    for file in `find "$QUERIES_DIR" -type f -regextype awk -regex ".*q${q}(v[0-9]+)?_.*"`; do
	bname=`basename $file`
	extension="${bname##*.}"
	fname="${bname%%.*}"
	case "$extension" in
	    "sql")
	    	for ((i = 0; i < RUNS; i++)); do
	    	    drop_psql_cache "$password"
	    	    PGPASSWORD=$pgpassword $PGDIR/psql -U `whoami` -h localhost -f "$file" >> "$ODIR/$fname.out" &
	    	    wait $!
	    	    echo -e "\n" >> "$ODIR/$fname.out"
	    	done
	    	wt=`awk '/Execution time/{sum += $3; n++} END {print sum/n}' "$ODIR/$fname.out"`
	    	;;
	    "js")
		for ((i = 0; i < RUNS; i++)); do
		    drop_mongo_cache "$password"
		    mongo --quiet "$file" >> "$ODIR/$fname.out" &
		    wait $!
		    echo -e "\n" >> "$ODIR/$fname.out"
		done
		wt=`awk -F'[:,]' '/executionTimeMillis"/{sum += $2; n++} END {print sum/n}' "$ODIR/$fname.out"`
		;;
        esac
	echo "$fname $wt" >> "$ODIR/results.csv"
    done
done
\end{lstlisting}

\end{document}
